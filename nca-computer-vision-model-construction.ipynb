{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-15T19:57:57.125564Z","iopub.execute_input":"2021-10-15T19:57:57.125944Z","iopub.status.idle":"2021-10-15T19:58:03.605418Z","shell.execute_reply.started":"2021-10-15T19:57:57.125852Z","shell.execute_reply":"2021-10-15T19:58:03.60434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a tensorflow training or validation generator function from the tensorflow data generator function, given a directory to interate over\ndef create_iterators(data_generator, directory, batch_size):\n    training_iterator = data_generator.flow_from_directory(\n        directory, # directory to get test or train data; must include both classes\n        class_mode = 'categorical', # labels generator images based on their directory of origin\n        color_mode = 'grayscale',\n        batch_size = batch_size,\n        subset = 'training'\n    )\n    validation_iterator = data_generator.flow_from_directory(\n        directory, # same directory as training data\n        class_mode='categorical',\n        color_mode = 'grayscale',\n        batch_size = batch_size,\n        subset='validation'\n    )\n    return training_iterator, validation_iterator","metadata":{"execution":{"iopub.status.busy":"2021-10-15T19:58:03.607141Z","iopub.execute_input":"2021-10-15T19:58:03.607817Z","iopub.status.idle":"2021-10-15T19:58:03.617364Z","shell.execute_reply.started":"2021-10-15T19:58:03.607768Z","shell.execute_reply":"2021-10-15T19:58:03.61559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator #creates the image data generator object\nfrom keras.models import Sequential #base model for neural network\nfrom keras.layers import InputLayer #input layer\nfrom keras.layers import Conv2D #finds relationships between local features for convolution\nfrom keras.layers import MaxPooling2D #reduces total number of parameters by effectively reducing resolution of image\nfrom keras.layers import Flatten #converts 3D matrix to 2D matrix for final Dense layer\nfrom keras.layers import Dropout #regularization layer\nfrom keras.layers import Dense #final output layer\nfrom tensorflow.keras.optimizers import Adam #momentum + learning rate decay\nfrom keras.losses import CategoricalCrossentropy #loss function #1\nfrom keras.metrics import CategoricalAccuracy #metric function #1\nfrom keras.metrics import AUC #metric function #2\nfrom keras.callbacks import EarlyStopping #save some processing cycles\n\n# design the model - hyperparameters here\ndef design_model(training_iterator, learning_rate):\n    # create neural network model\n    model = Sequential()\n    # create input layer\n    model.add(InputLayer(input_shape = training_iterator.image_shape))\n    # add first convolutional layer\n    model.add(Conv2D(\n        16,\n        16,\n        strides = 4, # same value for all dimensions in a 2D image\n        padding = 'same', #padding with zeros at edge of image\n        activation = 'relu' #relu activation\n    ))\n    # create first max pooling layer\n    model.add(MaxPooling2D(\n        pool_size = (2,2),\n        strides = (2,2),\n        padding = 'valid' # no filter padding for max pooling\n    ))\n    # create first dropout layer\n    model.add(Dropout(0.1))\n    # create second convolutional layer\n    model.add(Conv2D(\n        8,\n        8,\n        strides = 4,\n        padding = 'same',\n        activation = 'relu'\n    ))\n    # create second max pooling layer\n    model.add(MaxPooling2D(\n        pool_size = (2,2),\n        strides = (2,2),\n        padding = 'valid'\n    ))\n    # create second dropout layer\n    model.add(Dropout(0.1))\n    #flatten output for Dense layers\n    model.add(Flatten())\n    # create hidden layer\n    model.add(Dense(8, activation = 'relu'))\n    # create output layer\n    model.add(Dense(2, activation = 'softmax'))\n    \n    # create optimization function for model\n    opt = Adam(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.999)\n    \n    # compile model\n    model.compile(\n        optimizer = opt,\n        loss = CategoricalCrossentropy(),\n        metrics = [\n            CategoricalAccuracy(),\n            AUC()\n        ]\n    )\n    \n    # publish model summary, send model back to main body\n    model.summary()\n    return model\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-10-15T20:28:01.800584Z","iopub.execute_input":"2021-10-15T20:28:01.800879Z","iopub.status.idle":"2021-10-15T20:28:01.816141Z","shell.execute_reply.started":"2021-10-15T20:28:01.800847Z","shell.execute_reply":"2021-10-15T20:28:01.815083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_model(batch_size, learning_rate, num_epochs, weights):\n    print(\"Loading training & validation data\\n\")\n    # create the training generator\n    training_data_generator = ImageDataGenerator(\n        rescale = 1.0/255, #rescale pixel values from 0 to 255 to 0 to 1 for stability\n        zoom_range = 0.1, # database enhancement - zoom by up to 10%\n        rotation_range = 15, # database enhancement - rotate up to 15 degrees\n        width_shift_range = 0.05, #database enhancement - shift stuff by up to 5% width-wise\n        height_shift_range = 0.05, #database enhancement - shift stuff by up to 5% height-wise\n        validation_split = 0.2 #create validation generator as well\n    )\n    # get the training & validation iterators\n    training_iterator, validation_iterator = create_iterators(\n        training_data_generator, \n        '/kaggle/input/nca-computer-vision-project',\n        batch_size\n    )\n\n    print(\"Designing model\\n\")\n    # create the model\n    model = design_model(training_iterator, learning_rate)\n    # create an early stopping protocol just in case\n    stop = EarlyStopping(\n        monitor = 'val_loss', # monitor the validation loss to figure out when to stop training\n        mode = 'min', # seeks loss minimization\n        verbose = 1,\n        patience = 10 # number of epochs after validation loss stops getting better before it gives up\n    )\n    \n    print(\"Training model\\n\")\n    # fit the model\n    history = model.fit(\n        training_iterator, # the training data\n        steps_per_epoch = training_iterator.samples/batch_size,\n        epochs = num_epochs, # the number of epochs\n        verbose = 1, \n        callbacks = [stop], # add the early stopping protocol\n        validation_data = validation_iterator, # the test data\n        validation_steps = validation_iterator.samples/batch_size,\n        class_weight = weights\n    )\n    \n    return history, training_iterator, validation_iterator","metadata":{"execution":{"iopub.status.busy":"2021-10-15T20:28:03.683276Z","iopub.execute_input":"2021-10-15T20:28:03.684141Z","iopub.status.idle":"2021-10-15T20:28:03.697084Z","shell.execute_reply.started":"2021-10-15T20:28:03.684089Z","shell.execute_reply":"2021-10-15T20:28:03.696008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Model\nfrom tensorflow import argmax\nfrom tensorflow import expand_dims\n\n# visualizes convolutional layer activations\ndef visualize_activations(model, validation_iterator):\n    # output the fitted model's activations for convolutional layers, but not any other layers\n    activation_extractor = Model(\n        inputs = model.inputs,\n        outputs = [layer.output for layer in model.layers if \"conv2d\" in layer.name]\n    )\n    \n    # remove axes from plots to make them look cleaner\n    def clean_plot(plot):\n        plot.axes.get_xaxis().set_visible(False)\n        plot.axes.get_yaxis().set_visible(False)\n    \n    # create a dict mapping labels to categories - note: best guess\n    class_names = {\n        0:'High G',\n        1:'Low G'\n    }\n    \n    # grab 5 images with labels from validation iterator\n    sample_batch_input, sample_labels = validation_iterator.next()\n    sample_batch_input = sample_batch_input[0:5]\n    sample_labels = sample_labels[0:5]\n    \n    # make predictions using model\n    sample_predictions = model.predict(sample_batch_input)\n    \n    # iterate over each sample in the sample batch & show activations\n    for i, (image, prediction, label) in enumerate(zip(sample_batch_input, sample_predictions, sample_labels)):\n        image_name = f'Micrograph #{i}' # change this to file names later\n\n        # get predicted class with highest probability, then ground truth\n        predicted_class = argmax(prediction).numpy()\n        actual_class = argmax(label).numpy()\n        \n        # present results\n        print(image_name)\n        print(f'\\tModel Prediction: ({prediction} {class_names[actual_class]})')\n        print(f'\\tActual Class: ({actual_class} {class_names[actual_class]})')\n        print(f'\\tCorrect: {predicted_class == actual_class}')\n        \n        # show micrograph\n        sample_image = image\n        clean_plot(plt.imshow(\n            sample_image[:, :, 0],\n            cmap = 'gray'\n        ))\n        plt.title(image_name + f' Predicted {class_names[predicted_class]}, Actual {class_names[actual_class]}')\n        plt.tight_layout()\n        plt.show()\n        model_layer_output = activation_extractor(expand_dims(sample_image, 0))\n        plt.clf()\n        \n        # show the filters\n        for l_num, output_data in enumerate(model_layer_output):\n            # create a subplot for each filter\n            fig, axs = plt.subplots(1, output_data.shape[-1])\n            # for each filter\n            for i in range(output_data.shape[-1]):\n                clean_plot(\n                    axs[i].imshow(\n                        output_data[0][:, :, i],\n                        cmap = 'gray'\n                    )\n                )\n        \n        plt.suptitle(image_name + f' Conv {l_num}', y = 0.6)\n        plt.tight_layout()\n        plt.show()\n        plt.clf()\n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-15T20:28:04.26184Z","iopub.execute_input":"2021-10-15T20:28:04.262818Z","iopub.status.idle":"2021-10-15T20:28:04.277379Z","shell.execute_reply.started":"2021-10-15T20:28:04.262761Z","shell.execute_reply":"2021-10-15T20:28:04.276457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the accuracy reports\ndef visualize_accuracy(history):\n    # visualize test and train accuracy over time\n    fig = plt.figure()\n    ax1 = fig.add_subplot(2,1,1)\n    ax1.plot(history.history['categorical_accuracy'], label = 'Train')\n    ax1.plot(history.history['val_categorical_accuracy'], label = 'Test')\n    ax1.set_title('Model Accuracy')\n    ax1.set_xlabel('Number of Epochs')\n    ax1.set_ylabel('Categorical Accuracy')\n    ax1.legend()\n    \n    # plot auc and validation auc over time\n    keys = list(history.history.keys())\n    ax2 = fig.add_subplot(2, 1, 2)\n    ax2.plot(history.history[keys[2]], label = 'Train')\n    ax2.plot(history.history[keys[-1]], label = 'Test')\n    ax2.set_title('Model AUC')\n    ax2.set_xlabel('Number of Epochs')\n    ax2.set_ylabel('AUC')\n    ax2.legend()\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-15T20:28:04.778782Z","iopub.execute_input":"2021-10-15T20:28:04.779202Z","iopub.status.idle":"2021-10-15T20:28:04.787716Z","shell.execute_reply.started":"2021-10-15T20:28:04.77917Z","shell.execute_reply":"2021-10-15T20:28:04.786789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# create a report of the classes\ndef report(model, validation_iterator):\n    test_steps_per_epoch = np.math.ceil(validation_iterator.samples/validation_iterator.batch_size)\n    predictions = model.predict(validation_iterator, steps = test_steps_per_epoch)\n    predicted_classes = np.argmax(predictions, axis = 1)\n    true_classes = validation_iterator.classes\n    class_labels = list(validation_iterator.class_indices.keys())\n    \n    print(class_labels)\n    report = classification_report(true_classes, predicted_classes, target_names = class_labels)\n    print(report)\n    \n    cm = confusion_matrix(true_classes, predicted_classes)\n    print(cm)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T20:28:05.418633Z","iopub.execute_input":"2021-10-15T20:28:05.418915Z","iopub.status.idle":"2021-10-15T20:28:05.426105Z","shell.execute_reply.started":"2021-10-15T20:28:05.418884Z","shell.execute_reply":"2021-10-15T20:28:05.425134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 8\nLEARNING_RATE = 0.001\nEPOCHS = 50\nWEIGHTS = {0:1,1:1.65} # weights for different classes\n\nhistory, training_iterator, validation_iterator = fit_model(BATCH_SIZE, LEARNING_RATE, EPOCHS, WEIGHTS)\n\nmodel = history.model\nvisualize_activations(model, validation_iterator)\nvisualize_accuracy(history)\nreport(model, validation_iterator)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T20:28:06.10303Z","iopub.execute_input":"2021-10-15T20:28:06.10331Z","iopub.status.idle":"2021-10-15T20:31:37.93316Z","shell.execute_reply.started":"2021-10-15T20:28:06.103279Z","shell.execute_reply":"2021-10-15T20:31:37.93109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}